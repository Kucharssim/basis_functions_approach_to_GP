\documentclass[onecolumn,a4paper,11pt]{article}
\voffset -1cm
\hoffset -1cm
\textheight 22.3cm
\textwidth 15.0cm

\usepackage{blindtext}
\usepackage{titlesec}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{microtype}

\usepackage{lscape}

\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}

\usepackage{epstopdf}

\usepackage[numbers,sort&compress]{natbib}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{float}
\usepackage{soul}
\usepackage{xcolor}
\graphicspath{{./images/}}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}

\usepackage[titletoc]{appendix}

\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}


\bibpunct[, ]{[}{]}{,}{}{,}{,}
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}

\usepackage{listings}
\usepackage{slashbox}
\usepackage{caption}
\captionsetup{font=footnotesize}

\bibliographystyle{authordate1}


\title{Hilbert space approximate Gaussian process using Stan}


\begin{document}


\maketitle

\begin{abstract}
Gaussian processes are powerful non-parametric probabilistic models for stochastic functions, however they entail a complexity that is computationally intractable when the number of observations is large. In this paper, we focus on a novel approach for low-rank approximate Gaussian processes, based on the basis functions approximation via Laplace eigenfunctions for stationary covariance functions. The main contribution of this paper is the realization of a detailed analysis of the performance and practical implementation of the method in relation to key factors such as the number of basis functions, desired prediction space and non-linearity of the function to be learned. Intuitive visualizations and useful recommendations for the values of these factors which help users to improve performance and computation are provided. A diagnosis procedure of whether the chosen values for the number of basis functions and the desired prediction space are adequate to fit to the actual data is also proposed. Furthermore, we show the simplicity of the method, with an attractive computational complexity due to its linear structure, which makes it easier to be used as modular components in probabilistic programming frameworks. Several illustrative examples of the performance and applicability of the method in the Stan programming software are presented, and their Stan model codes are also provided.
\end{abstract}

\keywords{Gaussian process; Low-rank Gaussian process; Hilbert space methods; Sparse Gaussian process.}

\tableofcontents

\section{Introduction}\label{sec_bf_intro}

Gaussian processes (GPs) are flexible statistical models for specifying probability distributions over multi-dimensional non-linear functions \citep{rasmussen2006gaussian,neal1997monte}. Their name stems from the fact that any finite set of function values is jointly distributed as a multivariate Gaussian. GPs are defined by a mean and a covariance function. The covariance function encodes our prior assumptions about the functional relationship, such as continuity, smoothness, periodicity and scale properties. GPs not only allow for non-linear effects but can also implicitly handle interactions between input variables (covariates). Different types of covariance functions can be combined for further increased flexibility. Due to their generality and flexibility, GPs are of broad interest across machine learning and statistics \citep{rasmussen2006gaussian,neal1997monte}. Among others, they find application in the fields of spatial epidemiology \citep{diggle2013statistical,carlin2014hierarchical}, robotics and control \citep{deisenroth2015gaussian}, signal processing \citep{sarkka2013spatiotemporal}, as well as Bayesian optimization and probabilistic numerics \citep{roberts2010bayesian,briol2015probabilistic,hennig2015probabilistic}.

The key element of a GP is the covariance function that defines the dependence structure between function values at different inputs. However, the covariance function is also a computational issue because of the need of inverting its Gram matrix to optimize the hyperparameters. Given $n$ observations in the data, the computational complexity and memory requirements of exact GP implementation in general scale as $O(n^3)$ and $O(n^2)$, respectively. This limit their application to rather small data sets of a few tens of thousands observations at most. The problem becomes more severe when performing full Bayesian inference via sampling methods, where in each sampling step we need $O(n^3)$ computations when inverting the Gram matrix of the covariance function, usually through Cholesky factorization. To alleviate these computational demands, several approximate methods have been proposed. 

Sparse GPs are based on low-rank approximations of the covariance matrix. The low-rank approximation with $m \ll n$ {\it inducing points} implies reduced memory requirements of $O(nm)$ and corresponding computational complexity of $O(nm^2)$.
A unifying view on sparse GPs based on approximate generative methods
is provided in \cite{quinonero2005unifying}, while a general review
can be found in \cite{rasmussen2006gaussian}. \citet{Burt+Rasmussen+vanderWilk:2019} show that for regression with normally distributed covariates in $D$ dimensions and using the squared exponential covariance function, $M=O(\log^DN)$ is sufficient for accurate approximation.
% More recent developments in the context of sparse GPs include a structured kernel interpolation method \citep{wilson2015kernel}. More recently, \citep{bui2017unifying} proposed to use inducing points-based sparse approximation methods to perform GP approximations at inference time rather than at modeling time.

An alternative class of low-rank approximations is based on forming a basis function approximation with $m \ll n$ basis functions. The basis functions are usually presented explicitly, but can also be used to form a low rank covariance matrix approximation. Common basis function approximations rest on the spectral analysis and series expansions of GPs \citep{loeve1977probability,trees1968detection,adler1981geometry,cramer2013stationary}.
Sparse spectrum GPs are based on a sparse approximation to the frequency domain representation of a GP \citep{lazaro2010sparse,quia2010sparse,gal2015improving}. Recently, \cite{hensman2017variational} presented a variational Fourier feature approximation for GPs that was derived for the Mat{\'e}rn class of kernels. Another related method for approximating kernels relies on random fourier features \citep{rahimi2008random,rahimi2009weighted}.
Further, certain spline smoothing basis functions are equivalent to GPs with certain covariance functions \citep{wahba1990spline,Furrer+Nychka:2007}.
% , but not all spline models are GPs (e.g., some spline models are not marginalization consistent).

{\color{blue} Paul: Can we clarify how our discussed method relates to the former paragraph,
which introduces all sorts of basis function approximations. I.e. is our method
just one out of many or what makes it special? Perhaps we can somehow use the information provided
in: While Sparse Spectrum GP is based on a sparse spectrum, the reduced-rank method proposed in this paper aims to make the spectrum as ‘full’ as possible at a given rank. Recent Splines models can reproduce the Matern family of covariance functions (see, e.g., \cite{wood2003thin}), however our approach can reproduce basically all of the stationary covariance functions.}

Furthermore, a recent related work based on a spectral representation of GPs as an infinite series expansion with the Karhunen-Loève representation (see, e.g. \cite{grenander1981abstract}) can be found in \cite{JSSv090i10}.

In this paper we propose an approximate framework for fast and accurate inference for GPs. We focus on the basis function approximation via Laplace eigenfunctions for stationary covariance functions proposed by \citet{solin2018hilbert}. Using basis function expansion, a GP is approximated with a linear model which makes inference considerably faster. A linear model structure makes GPs easier to implement as building blocks in more complicated models in modular probabilistic programming frameworks, where there is a big benefit if approximation specific computation is simple. Furthermore, a linear representation of a GP makes it easier to be used as latent function
in non-Gaussian observational models allowing modelling flexibility. The basis function approximation via Laplace eigenfunctions can be made arbitrary accurate and the trade-off between computational complexity and approximation accuracy can easily be controlled.

The Laplace eigenfunctions can be computed analytically and they are independent of the particular choice of the covariance kernel including the hyperparameters. While the pre-computation cost of the basis functions is $O(m^2n)$, the computational cost of learning the covariance function parameters is $O(mn+m)$ in every step of the optimizer. This is a big advantage in terms of speed for iterative algorithms such as Markov chain Monte Carlo (MCMC). Another advantage is the reduced memory requirements of automatic differentation methods used in modern probabilistic programming frameworks, such as Stan \citep{carpenter2017stan}, WinBUGS \citep{lunn2000winbugs} and others. This is because the memory requirements of automatic differentation rather scale with the computational complexity instead of with the usual memory requirements for the posterior density computation. The basis function approach also provides an easy way to apply the non-centered parameterization of GPs, which reduces the posterior dependency between parameters representing the estimated function and the hyperparameters of the covariance function, which further improves MCMC efficiency.

%\textcolor{red}{Aki: I moved this from section 2, needs to be combined with other intro}
%We propose an approximate framework for fast and accurate inference
%for GPs. Using a basis function expansion, we
%approximate the GPs with a linear model. This
%representation has three main advantages: 1) it makes inference
%considerably faster due to the linear structure, 2) it is simple to
%implement, which makes it easy to use GPs as building
%blocks in more complicated models and can be used as latent function
%in non-Gaussian observational models allowing modelling flexibility,
%3) it can be made arbitrary accurate and the trade-off between
%computational complexity and approximation accuracy can easily be
%controlled.

While \citet{solin2018hilbert} have fully developed the mathematical theory behind this specific approximation of GPs, further work is needed for its practical implementation in probabilistic programming frameworks. In this paper, the relations going on among the key factors of the method such as the number of basis functions, desired prediction space, or properties of the true functional relationship between covariates and response variable, are investigated and analyzed in detail in relation to the performance and accuracy of the method. Practical recommendations for the values of the key factors, based on useful graphs that encode the recognized relationships, are given. Which will help users to chose valid and optimized values for these factors, improving computational performance and saving time of computation. A diagnosis of whether the chosen values for the number of basis functions and the desired prediction space are adequate to fit to the actual data is proposed.

{\color{blue} (Gabi: This paragraph is the old version of the former paragraph) While \citet{solin2018hilbert} have fully developed the mathematical theory behind this specific approximation of GPs, further work is needed for its practical implementation in probabilistic programming frameworks. In this paper, we analyze in detail the performance and accuracy of the method in relation to key factors such as the number of basis functions, desired prediction space, or properties of the true functional relationship between covariates and response variable. We provide intuitive visualizations and practical recommendations for the choice of these factors, which will help users to improve computational performance while maintaining close approximation to exact GPs.}

The approach is implemented in a fully probabilistic framework and for the Stan programming probabilistic software. This work has served as a basis for the subsequent implementation of the method in the \textit{brms} library \citep{burkner2017brms} of the R software \citep{R2014R}. Several illustrative examples, simulated and real datasets, of the performance and applicability of the model, and accompanied by their Stan model codes, are developed. 

Although there are several GP specific software packages available to date (GPML \citep{rasmussen2010gpml},  GPstuff \citep{vanhatalo2013gpstuff}, GPy \citep{gpy2014}, GPflow \citep{GPflow2017}), each provide efficient implementations only for a restricted range of GP based models. In this paper, we do not focus on the fastest possible inference for some specific GP models, but instead are interested in how GPs can be easily used as modular components in probabilistic programming frameworks. 

The remainder of the paper is structured as follows. In Section \ref{ch4_gp}, we introduce GPs with the covariance and spectral density functions. In Section \ref{ch5_sec_method}, the reduced rank approximation to GPs proposed by \cite{solin2018hilbert} is described. In Section \ref{ch5_sec_accuracy}, the accuracy of these approximations under several conditions using analytical and numerical methods is analyzed. Several case studies in which we fit
exact and approximate GPs to real and simulated data are provided in Sections \ref{ch5_sec_univariate_cases} and \ref{ch5_sec_multivariate_cases}. We end with a discussion in Section \ref{ch5_sec_conclusion}. Appendix \ref{ch5_app_approx_covfun} includes a brief presentation of the mathematical details behind this Hilbert space approximation of a stationary covariance function, and Appendix \ref{ch5_sec_periodic} presents a low-rank representation of a GP for the particular case of a periodic covariance function. Additional content as online complementary material with more case studies illustrating the performance and applicability of the method can be found through the following link:


\section{Gaussian process as a prior}\label{ch4_gp}

A GP is a stochastic process which defines the distribution over a collection of random variables indexed by a continuous variable, i.e. $\left\lbrace f(t): t \in \mathcal{T}\right\rbrace$ for some index set $\mathcal{T}$. GPs have the defining property that the marginal distribution of any finite subset of random variables, $\left\lbrace f(t_1), f(t_2), \hdots, f(t_K) \right\rbrace$, is a multivariate Gaussian distribution.

In this work, GPs will take the role of a prior distribution over function spaces for non-parametric latent functions in a Bayesian setting. 
Consider a data set $\mathcal{D} = \left\lbrace \bm{x}_n, y_n \right\rbrace_{n=1}^N$, where $y_n$ is modelled conditionally as $p(y_n|f(\bm{x}_n),\phi)$, where $p$ is some parametric distribution with parameters $f$ and $\phi$, and $f$ is an unknown function with GP prior, which depends on an input $\bm{x}_n\in {\rm I\!R}^D$. This generalizes trivially to more complex models depending on several unknown functions, for example such as $p(y_n|f(\bm{x}_n),g(\bm{x}_n))$ or multilevel models. Our goal is to obtain posterior distribution for the value of the function $\tilde{f}=f(\tilde{\bm{x}})$  evaluated at a new input point $\tilde{\bm{x}}$.
% That is, we want to obtain the predictive distribution $p(f^*|\mathcal{D})$ of $f^*$ conditioned on the data $\mathcal{D}$.\\

We assume a GP prior for $f \sim \mathcal{GP}(\mu(\bm{x}), k(\bm{x}, \bm{x}'))$, where $\mu: {\rm I\!R}^D \rightarrow {\rm I\!R}$ and $k: {\rm I\!R}^D \times {\rm I\!R}^D \rightarrow {\rm I\!R}$ are the mean and covariance functions, respectively,
%
\begin{align*}
 	\mu(\bm{x}) &= \mathbb{E}\!\left[f(\bm{x})\right],\\ 
 	k(\bm{x}, \bm{x}') &= \mathbb{E}\!\left[\left( f(\bm{x}) - \mu(\bm{x}) \right)\left( f(\bm{x}') - \mu(\bm{x}') \right)\right].
\end{align*} 

The mean and covariance functions completely characterize the GP prior, and control the a priori behavior of the function $f$. Let $\bm{f}=\left\lbrace f(\bm{x}_n) \right\rbrace_{n=1}^N$, then the resulting prior distribution for $\bm{f}$ is a multivariate Gaussian distribution
$\bm{f} \sim \mathcal{N}(\bm{\mu}, \bm{K})$,
 where $\bm{\mu} = \left\lbrace \mu(\bm{x}_n) \right\rbrace_{n=1}^N$ is the mean and $\bm{K}$ the covariance matrix, where $K_{i,j}=k(\bm{x}_i,\bm{x}_j)$. The covariance function $k(\bm{x}, \bm{x}')$ might depend on a set of hyperparameters, $\bm{\theta}$, but we will not write this dependency explicitly to ease the notation.
The joint distribution of $\bm{f}$ and a new $\tilde{f}$ is also a multivariate Gaussian as,
%
\begin{align*}
p(\bm{f}, \tilde{f})=\mathcal{N} \left( \left[ \begin{array}{cc}
\bm{f} \\ 
f^*
\end{array} \right] \,\middle|\, \bm{0},\left[ \begin{array}{cc}
\bm{K}_{\bm{f},\bm{f}} & \bm{k}_{\bm{f},\tilde{f}} \\ 
\bm{k}_{\tilde{f},\bm{f}} & k_{\tilde{f},\tilde{f}}
\end{array} \right] \right),
\end{align*} 

\noindent where $\bm{k}_{\bm{f},\tilde{f}}$ is the covariance between $\bm{f}$ and $\tilde{f}$, and $k_{\tilde{f},\tilde{f}}$ is the prior variance of $\tilde{f}$. %By using the conditioning properties of multivariate Gaussian distributions, we can derive the predictive distribution for $f^*$  given $\bm{f}$ analytically,
% \begin{align*}
	% p(f^*| \bm{f}) = \mathcal{N}(f^*| \bm{k}_{f^*,\bm{f}} \bm{K}_{\bm{f},\bm{f}}^{-1} \bm{f},  k_{f^*,f^*}-\bm{k}_{f^*,\bm{f}} \bm{K}_{\bm{f},\bm{f}}^{-1} \bm{k}_{\bm{f},f^*}  )
% \end{align*}

% {\color{red} Aki: the following until ``While Gaussian'' seems too basic to be in the paper:}

% The joint distribution of the observations $\bm{y} = \left\lbrace y_n \right\rbrace_{n=1}^N$ and function values $\bm{f}$ and $f^*$, $p(\bm{y}, \bm{f}, f^*)$, is the product of the conditional distribution for $\bm{y}$ given $\bm{f}$ and the joint distribution for $\bm{f}$ and $f^*$,
% \begin{align*}
% 	p(\bm{y}, \bm{f}, f^*) = p(\bm{y}|\bm{f})p(\bm{f}, f^*)
% \end{align*}
 
% By marginalizing over $\bm{f}$ and conditioning on vector of observations $\bm{y}$, we obtain the posterior distribution of interest
% %
% \begin{align}\label{eq:posterior}
% 	p(f^*|\bm{y}) = \frac{\int p(\bm{y}|\bm{f})p(\bm{f}, f^*)\text{d}\bm{f}}{p(\bm{y})},
% \end{align}
% where $p(\bm{y})$ is the marginal likelihood and is given by
% %
% \begin{align}\label{eq:evidence}
% 	p(\textbf{y}) = \int p(\bm{y}|\bm{f})p(\bm{f}, f^*) \text{d}\bm{f}\text{d}f^*.
% \end{align}
% %
% If the observation model $p(\bm{y}|\bm{f})$ is Gaussian, both integrals in eq. \eqref{eq:posterior} and \eqref{eq:evidence} can be solved analytically conditioned on the hyperparameters. For example, an isotropic Gaussian likelihood yields the following closed-form solution
% \begin{align*}
% 	p(f^*| \bm{y}) = \mathcal{N}(f^*| \textbf{k}_{f^*,\bm{f}} (\textbf{K}_{\bm{f},\bm{f}} + \sigma^2 \bm{I})^{-1} \bm{f},  k_{f^*,f^*}-\bm{k}_{f^*,\bm{f}} (\bm{K}_{\bm{f},\bm{f}} + \sigma^2 \bm{I})^{-1} \bm{k}_{\bm{f},f^*}  ),
% \end{align*}
% where $\sigma^2$ is the noise variance.

If $p(y_n|f(\bm{x}_n),\phi)=N(y_n|f(\bm{x}_n),\sigma)$ then $\bm{f}$ can be integrated out analytically (with a computational cost of $O(n^3)$ for exact GP and $O(nm^2)$ for sparse GP). If $p(y_n|f(\bm{x}_n),\phi)=N(y_n|f(\bm{x}_n),g(\bm{x}_n))$ or $p(y_n|f(\bm{x}_n),\phi)$ is non-Gaussian, the marginalization does not have closed form solution. Furthermore, if a prior distribution is imposed on $\phi$ and $\bm{\theta}$ to form a joint posterior for $\phi$, $\bm{\theta}$ and $\bm{f}$, approximate inference such as Markov chain Monte Carlo (MCMC) \citep{brooks_2011}, Laplace approximation (\citep{williams1998bayesian,rasmussen2006gaussian}, expectation propagation \citep{minka2001expectation}, or variational Bayes methods \citep{gibbs2000variational,csato2000efficient} need to be used. In this paper we focus on use of MCMC for integrating over the joint
posterior. MCMC is not usually the fastest approach, but allows
accurate inference for general models in probabilistic programming
setting. We consider the computational costs of GPs specifically from
this point of view.

\subsection{Covariance function and spectral density}\label{ch4_sec_cov}

The covariance function is the crucial ingredient in a GP as it encodes our prior assumptions about the function, and defines a correlation structure which characterize the correlations between function values at different inputs. A covariance function needs be symmetric and positive semi-definite \citep{rasmussen2006gaussian}. A stationary covariance function is a function of $\bm{\tau}=\bm{x}-\bm{x}' \in {\rm I\!R}^D$, such that it can be written $k(\bm{x},\bm{x}') = k(\bm{\tau})$, which means that the covariance is invariant to translations. Isotropic covariance functions are those that are function of the distance between observations, $k(\bm{x},\bm{x}') = k(|\bm{x}-\bm{x}'|) = k(r), r\in {\rm I\!R}$, which means that the covariance is both translation and rotation invariant. The most commonly used distance between observations is the norm L2 $(|\bm{x}-\bm{x}'|_{L2})$, also known as Euclidean distance, although other types of distances can be considered. 

The Mat\'ern class of isotropic covariance functions is given by, 
%
\begin{align*}
k_{\nu}(r)&=\sigma^2 \, \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}r}{\ell}\right)^{\nu}K_{\nu}\left(\frac{\sqrt{2\nu}r}{\ell}\right),
\end{align*}

\noindent where $\nu$ is the order the kernel, $K_{\nu}$ the modified Bessel function of the second kind, and the $\ell$ and $\sigma$ are the length-scale and magnitude (marginal variance), respectively, of the kernel. The particular case where $\nu=\infty$ and $\nu=3/2$ are probably the most commonly used kernels \citep{rasmussen2006gaussian}, 
%
\begin{align*}
k_{\infty}(r)&=\sigma^2 \exp\bigg(-\frac{1}{2} \frac{r^2}{\ell^2}\bigg),  \\
k_{\frac{3}{2}}(r)&=\sigma^2\bigg(1+\frac{\sqrt{3}r}{\ell}\bigg) \exp\bigg(-\frac{\sqrt{3}r}{\ell}\bigg). 
\end{align*}

\noindent The former is commonly known as squared exponential (exponentiated quadratic) covariance function. Assuming the Euclidean distance between observations, $r=|\bm{x}-\bm{x}'|_{L2}=\sqrt{\sum_{i=1}^{D}(x_i-x_i')^2}$, the kernels written above take the form:
%
\begin{align*}
k_{\infty}(|\bm{x}-\bm{x}'|_{L2})&=\sigma^2 \exp\bigg(-\frac{1}{2} \sum_{i=1}^{D}\frac{(x_i-x_i')^2}{\ell_i^2}\bigg),\\
k_{\frac{3}{2}}(|\bm{x}-\bm{x}'|_{L2})&=\sigma^2 \Bigg(1+\sqrt{\sum_{i=1}^{D}\frac{3(x_i-x_i')^2}{\ell_i^2}}\Bigg)\exp\Bigg(- \sqrt{\sum_{i=1}^{D}\frac{3(x_i-x_i')^2}{\ell_i^2}}\Bigg). \\
\end{align*}

\noindent Notice that the previous expressions have been easily generalized to using a multidimensional length-scale $\bm{\ell}\in {\rm I\!R}^D$. The use of a multidimensional length-scale basically turns the isotropic covariance function into non-isotropic. 

Stationary covariance functions can be represented in terms of their spectral densities \citep{rasmussen2006gaussian}. In this sense, the covariance function of a stationary process can be represented as the Fourier transform of a positive finite measure (\textit{Bochner's theorem}, see, e.g. \cite{akhiezer1993theory}). If this measure has a density, it is known as the spectral density of the covariance function, and the covariance function and the spectral density are Fourier duals, known as the \textit{Wiener-Khintchine theorem} \citep{rasmussen2006gaussian}. The spectral density functions associated with the Mat\'ern class of covariance functions is given by
%
\begin{align*}
S_{\nu}(\omega)&= \sigma^2 \frac{2^D\pi^{D/2}\Gamma(\nu+D/2)(2\nu)^{\nu}}{\Gamma(\nu)l^{2\nu}}\left(\frac{2\nu}{l^2}+4\pi^2\omega^2 \right)^{(\nu+D/2)}
\end{align*}

\noindent in $D$ dimensions, where variable $\omega\in {\rm I\!R}$ is a distance in the frequency domain, and $\ell$ and $\sigma$ are the lengthscale and magnitud (marginal variance), respectively, of the kernel. The particular cases where $\nu=\infty$ and $\nu=3/2$ take the form:
%
\begin{align}
S_{\infty}(\omega)&= \sigma^2 \sqrt{2\pi}^D  \ell^D  \exp\left(-0.5 \ell^2 \omega^2\right), \label{eq_specdens_inf}  \\
S_{\frac{3}{2}}(\omega)&= \sigma^2 \frac{2^D\pi^{D/2}\Gamma(\frac{D+3}{2})\sqrt{3}^3}{\frac{1}{2}\sqrt{\pi}\ell^3}\left(\frac{3}{\ell^2}+\omega^2 \right)^{-\frac{D+3}{2}}. \label{eq_specdens_32} 
\end{align}

\noindent Particularizing to an input dimension $D=3$ and Euclidean distance $\omega=\sqrt{\sum_{i=1}^{3}s_i^2}$, and considering a multidimensional lengthscale $\bm{\ell} \in {\rm I\!R}^{3}$, the spectral densities written above take the form:
%
\begin{align*}
S_{\infty}(\omega)&= \sigma^2 \sqrt{2\pi}^{3}  \prod_{i=1}^{3} \ell_i  \mathrm{exp}\left(-\frac{1}{2} \sum_{i=1}^{3} \ell_i^2 s_i^2 \right),   \\
S_{\frac{3}{2}}(\omega)&= \sigma^2 32\pi\sqrt{3}^3\prod_{i=1}^{3}\ell_i\left(3+\sum_{i=1}^{3}\ell_i^2 s_i^2 \right)^{-3}.
\end{align*}


\section{Hilbert space approximate Gaussian process model}\label{ch5_sec_method}

The approximate GP method, developed by \cite{solin2018hilbert} and implemented in this paper, is based on considering the covariance operator of a homogeneous (stationary) covariance function as a pseudo-differential operator constructed as a series of Laplace operators. Then, the pseudo-differential operator is approximated with Hilbert space methods on a compact subset $\Omega \subset {\rm I\!R}^D$ subject to some boundary condition. For brevity, we will refer to these approximate Gaussian processes as HSGPs. Below, we will present the main results around HSGPs relevant for practical application. More details and mathematical proofs are provided in \cite{solin2018hilbert}. Our starting point for presenting the method is the main result obtained by \cite{solin2018hilbert} of the definition of the covariance function as a series expansion of eigenvalues and eigenfunctions of the Laplacian operator. The mathematical details of this approximation have been briefly presented in the Appendix \ref{ch5_app_approx_covfun} of this paper.

We begin by focusing on the case of a unidimensional input space (i.e., on GPs with just a single covariate) such that $\Omega \in [-L,L] \subset {\rm I\!R}$, where $L$ is some positive real value to which we also refer as boundary condition. As $\Omega$ describes the interval in which the approximations are valid, $L$ plays a critical role in the accuracy of HSGPs. We will come back to this issue in Section \ref{ch5_sec_accuracy}.

Within $\Omega$, we can write any stationary covariance function with input values $\{x,x'\} \in \Omega$ as
%
\begin{equation}\label{eq_approxcov}
k(x,x') = \sum_{j=1}^\infty S_{\theta}(\sqrt{\lambda_j}) \phi_j(x) \phi_j(x'),
\end{equation} 

\noindent where $S_{\theta}$ is the spectral density of the stationary covariance function $k$ (see Section \ref{ch4_sec_cov}) and $\theta$ the set of hyperparameters of $k$ \citep{rasmussen2006gaussian}. The terms $\{\lambda_j\}_{j=1}^{\infty}$ and $\{\phi_j(x)\}_{j=1}^{\infty}$ are the sets of eigenvalues and eigenvectors, respectively, of the Laplacian operator in the given domain $\Omega$. Namely, they satisfy the following eigenvalue problem in $\Omega$ when applying the Dirichlet boundary condition (other boundary conditions could be used as well):
%
\begin{align}\label{eq_eigenproblem}
\begin{split}
-\nabla^2 \phi_j(x)&=\lambda \phi_j(x), \hspace{1cm}  x \in \Omega \\ 
\phi_j(x)&= 0, \hspace{1.85cm}   x \notin \Omega.
\end{split}
\end{align} 

\noindent The eigenvalues $\lambda_j>0$ are real and positive because the Laplacian is a positive definite Hermitian operator, and the eigenfunctions $\phi_j$ for the eigenvalues problem in equation (\ref{eq_eigenproblem}) are sinusoidal functions. Independently of the covariance function, they can be computed as
%
\begin{align}
\lambda_j&=\left(\frac{j\pi}{2L}\right)^2, \label{eq_eigenvalue}\\
\phi_j(x)&=\sqrt{\frac{1}{L}} \text{sin}\left(\sqrt{\lambda_j}(x+L)\right). \label{eq_eigenfunction}
\end{align}

If we truncate the sum in (\ref{eq_approxcov}) to the first $m$ terms, the approximate covariance function becomes
%
\begin{equation}
k(x,x') \approx \sum_{j=1}^m S_{\theta}(\sqrt{\lambda_j}) \phi_j(x) \phi_j(x') = \bm{\phi}(x)^\intercal \Delta \bm{\phi}(x'), \nonumber
\end{equation}

\noindent where $\bm{\phi}(x)=\{\phi_j(x)\}_{j=1}^{m} \in {\rm I\!R}^{m}$ is the column vector of basis functions, and $\Delta  \in {\rm I\!R}^{m\times m}$ is the diagonal matrix of the spectral densities $S_{\theta}(\sqrt{\lambda_j})$: 
%
\begin{eqnarray}
\Delta &=&  \begin{bmatrix}
    S_{\theta}(\sqrt{\lambda_1}) & & \\
    & \ddots & \nonumber \\
    & & S_{\theta}(\sqrt{\lambda_m}) \\
  \end{bmatrix}.
\end{eqnarray}

Thus, the Gram matrix $\text{K}$ of the covariance function $k$ for a set of observations $i=1,\ldots,n$ and corresponding input values $\{x_i\}_{i=1}^{n} \in \Omega^{n}$ can be represented as
%
\begin{equation}
\text{K}= \Phi \Delta \Phi^\intercal, \nonumber
\end{equation}

\noindent where $\Phi \in {\rm I\!R}^{n\times m}$ is the matrix of eigenfunctions $\phi_j(x_i)$:
%
\begin{eqnarray}
\Phi &=&  \left[ {\begin{array}{ccc}
   \phi_1(x_1) & \cdots & \phi_m(x_1)  \\
    \vdots &\ddots & \vdots  \nonumber \\ 
    \phi_1(x_n) & \cdots & \phi_m(x_n) \\
  \end{array} } \right].
\end{eqnarray}
 
\noindent As a result, the model for $f$ can be written as
%
\begin{equation}
\bm{f} \sim \mathcal{N}(\bm{\mu},\Phi \Delta \Phi^\intercal). \nonumber
\end{equation}

\noindent This equivalently leads to a linear representation of $f$ via
%
\begin{equation}\label{eq_approxf}
f(x) \approx \sum_{j}^m \left( S_{\theta}(\sqrt{\lambda_j})\right)^{1/2} \phi_j(x) \beta_j,
\end{equation}

\noindent where $\beta_j \sim \text{Normal}(0,1)$. Thus, the function $f$ is approximated with a finite basis function expansion (using the eigenfunctions $\phi_j$ of the Laplace operator), scaled by the square root of spectral density values. A key property of this approximation is that the eigenfunctions $\phi_j$ do not depend on the covariance hyperparameters $\theta$. Instead, the only dependence of the model on $\theta$ is through the spectral density $S_{\theta}$. The eigenvalues $\lambda_j$ are monotonically increasing with $j$ and $S_{\theta}$ goes rapidly to zero for bounded covariance functions. Therefore, equation (\ref{eq_approxf}) can be expected to be a good approximation for a finite number of $m$ terms in the series as long as the inputs values $x_i$ are not too close to the boundaries $-L$ and $L$ of $\Omega$. The computational cost of univariate HSGPs scales as $O(nm + m)$, where $n$ is the number of observations and $m$ the number of basis functions.

The parameterization in equation (\ref{eq_approxf}) is naturally in the non-centered parameterization form with independent prior distribution on $\beta_j$, which makes the posterior inference easier. Furthermore, all dependencies on the covariance kernel and the hyperparameters is through the prior distribution of the regression weights $\beta_j$. The parameter posterior distribution $p(\bm{\beta}|\bm{y})$ is $m$-dimensional, where $m$ is much smaller than the number of observations $n$. Therefore, the parameter space is greatly reduced and this makes inference faster, especially when sampling methods are used.

\subsection{Generalization to multidimensional GPs} \label{ch5_sec_method_multi}

The results from the previous section can be generalizes to a multidimensional input space with compact regular domain $\Omega=[-L_1,L_1] \times \dots \times [-L_d,L_d]$ and Dirichlet boundary conditions. 
In a $D$-dimensional input space, the total number of eigenfunctions and eigenvalues in the approximation is equal to the number of $D$-tuples, that is possible combinations of univariate eigenfunctions over all dimensions. The number of $D$-tuples is given by 
%
\begin{align} \label{eq_m_multi}
m^{\ast} = \prod_{d=1}^{D} m_d,
\end{align}

\noindent where $m_d$ is the number of basis function for the dimension $d$. Let $\mathbb{S}\in {\rm I\!N}^{m^{\ast} \times D}$ be the matrix of all those $D$-tuples. For example, suppose we have $D=3$ dimensions and use $m_{1}=2$, $m_{2}=2$ and $m_{3}=3$ eigenfunctions and eigenvalues for the first, second and third dimension, respectively. Then, the number of multivariate eigenfunctions and eigenvalues is $m^{\ast} = m_{1} \cdot m_{2} \cdot m_{3} = 12$ and the matrix $\mathbb{S}\in {\rm I\!N}^{12 \times 3}$ is given by
\begin{align}\small
\mathbb{S}=
\left[ {\begin{array}{ccc}
1 & 1 & 1 \nonumber \\
1 & 1 & 2 \\
1 & 1 & 3 \\
1 & 2 & 1 \\
1 & 2 & 2 \\
1 & 2 & 3 \\
2 & 1 & 1 \\
2 & 1 & 2 \\
2 & 1 & 3 \\
2 & 2 & 1 \\
2 & 2 & 2 \\
2 & 2 & 3 
\end{array} } \right].
\end{align} 

Each multivariate eigenfunction $\phi^{\ast}_j$ corresponds to the product of the univariate eigenfunctions whose indices corresponds to the elements of the $D$-tuple $\mathbb{S}_{j\cdotp}$, and each multivariate eigenvalue $\bm{\lambda}^{\ast}_j$ is a $D$-vector with elements that are the univariate eigenvalues whose indices correspond to the elements of the $D$-tuple $\mathbb{S}_{j\bm{\cdotp}}$. Thus, for $\bm{x}=\{x_d\}_{d=1}^D \in \Omega$ and $j=1,\ldots,m^{\ast}$, we have: 
%
\begin{align}
\bm{\lambda}^{\ast}_j &= \left\{ \lambda_{\mathbb{S}_{jd}} \right\}_{d=1}^D =  \left\{ \left(\tfrac{\pi \mathbb{S}_{jd}}{2L_d}\right)^2 \right\}_{d=1}^D, \label{eq_eigenvalue_multi} \\
%
\phi^{\ast}_j(\bm{x}) &= \prod_{d=1}^{D} \phi_{\mathbb{S}_{jd}}(x_d) = \prod_{d=1}^{D} \sqrt{\frac{1}{L_d}} \text{sin}\left(\sqrt{\lambda_{\mathbb{S}_{jd}}}(x_d+L_d)\right). \label{eq_eigenfunction_multi}
\end{align}

\noindent The approximate covariance function is then represented as
%
\begin{equation}\label{eq_approxcov_multi}
k(\bm{x},\bm{x}') \approx \sum_{j=1}^{m^{\ast}} 
S^{\ast}_{\theta}\left(\sqrt{\bm{\lambda}^{\ast}_j}\right)
\phi^{\ast}_j(\bm{x}) \phi^{\ast}_j(\bm{x}'),
\end{equation}

\noindent where $S^{\ast}_{\theta}$ is the spectral density of the $D$-dimensional covariance function (see Section \ref{ch4_sec_cov}). We can now write the approximate series expansion of the multivariate function $f$ as
%
\begin{equation}\label{eq_approxf_multi}
f(\bm{x}) \approx \sum_{j=1}^{m^{\ast}} 
\left( S^{\ast}_{\theta} \left(\sqrt{\bm{\lambda}^{\ast}_j} \right)\right)^{1/2}
\phi^{\ast}_j(\bm{x}) \beta_j, 
\end{equation}

\noindent where, again, $\beta_j \sim \text{Normal}(0,1)$. The computational cost, in learning the covariance function hyperparameters, of multivariate HSGPs scales as $O(n m^{\ast} + m^{\ast})$, where $n$ is the number of observations and $m^{\ast}$ is the number of multivariate basis functions. Although this still implies linear scaling in $n$, the approximation is more costly than in the univariate case, as $m^{\ast}$ is the product of the number of univariate basis functions over the input dimensions and grows exponentially with respect to the number of  dimensions.


\section{The accuracy of the approximation}\label{ch5_sec_accuracy}

The accuracy and speed of the HSGP model depends on several interrelated factors, most notably on the number of basis functions and on the boundary condition of the Laplace eigenfunctions. Furthermore, appropriate values for these factors will depend on the non-linearity of the estimated function, which is in turn characterized by the lengthscale of the covariance function.
In this section, we analyze the effects of the number of basis functions and the boundary condition on the approximation accuracy. We present recommendations on how they should be chosen and diagnostics to check the accuracy of the obtained approximation. 

Ultimately, these recommendations lie on the relationships among the number of basis functions, the boundary factor and the lengthscale of the function, which depend on the particular choice of the kernel function. In this work we built these relationships for the square exponential covariance function and Matern ($\nu$=3/2) covariance function in the present section, and for the periodic squared exponential covariance function in Appendix \ref{ch5_sec_periodic}. For other kernels, the relationships will be slightly different, in function of mainly the smoothness or wigglyness of the kernel effects.

\subsection{Dependency on the number of basis functions and the boundary condition} \label{ch5_subsec_dependency}

As explained in Section \ref{ch5_sec_method}, the approximation of the covariance function is a series expansion of eigenfunctions and eigenvalues of the Laplace operator in a given domain $\Omega$, for instance in a one-dimensional input space $\Omega=[-L,L]\subset {\rm I\!R}$:
%
\begin{equation*}
k(\tau) = \sum_{j=1}^{\infty} S_{\theta} \left(\sqrt{\lambda_j} \right) \phi_j(\tau) \phi_j(0), 
\end{equation*} 

\noindent where $L$ describes the boundary condition, $j$ is the index for the eigenfunctions and eigenvalues, and $\tau=x-x'$ is the difference between two covariate values $x$ and $x'$ in $\Omega$. The eigenvalues $\lambda_j$ and eigenfunctions $\phi_j$ are given in equations (\ref{eq_eigenvalue}) and (\ref{eq_eigenfunction}) for the unidimensional case and in equations (\ref{eq_eigenvalue_multi}) and (\ref{eq_eigenfunction_multi}) for the multidimensional case. The number of basis functions can be truncated at some finite positive value $m$ such that the difference between the densities of the exact and approximate covariance functions is less than a predefined threshold $\varepsilon > 0$:
%
\begin{eqnarray}\label{eq_diff_covs}
\int k(\tau) \, d\tau - 
\int \sum_{j=1}^m S_{\theta}\left(\sqrt{\lambda_j} \right) \phi_j(\tau) \phi_j(0) \, d\tau < \varepsilon.
\end{eqnarray}

The finite number $m$ of basis functions in the approximation needed to satisfy equation (\ref{eq_diff_covs}) depends on the non-linearity of the function to be learned, that is on its lengthscale $\ell$, which constitutes a hyperparameter of the GP. The approximation also depends on the boundary $L$ (see equations (\ref{eq_eigenvalue}), (\ref{eq_eigenfunction}), (\ref{eq_eigenvalue_multi}) and (\ref{eq_eigenfunction_multi})), which will affect its accuracy especially near the boundaries. As we will see later on, $L$ will also influence the number of basis functions required in the approximation. In the present paper, we will set $L$ an extension of the desired covariate input domain $\Psi$. Without loss of generality, we can assume $\Psi$ to be symmetric around zero, that is $\Psi=[-S,S] \subset \Omega$. We now define $L$ as
%
\begin{equation}\label{eq_boundary}
L=c \cdot S,
\end{equation} 

\noindent where $S$ (for $S > 0$) represents the half-range of the input space, and $c$ (for $c \geq 1$) is the proportional extension factor. In the following, we will refer to $c$ as the boundary factor of the approximation. The boundary factor can also be regarded as the boundary $L$ normalized by the half-range $S$ of the input space.

We start with an illustration on how the number of basis functions $m$ and boundary factor $c$ influences the accuracy of the HSGP approximations, separately. For this purpose, a set of noisy observations are drawn from an exact GP model with lengthscale $\ell=0.3$ and marginal variance $\alpha=1$ of the kernel function, using input values from the zero-mean input domain with half-range $S=1$. Several HSGP models with varying $m$ and $L$ are fitted to this data. In this example, the lengthscale and marginal variance parameters used in the HSGPs are fixed to the true values of the data-generating model. 
Figures \ref{fig1_Post_J} and \ref{fig2_Post_L} illustrate the individual effects of $m$ and $c$, respectively, on the posterior predictions of the estimated function and on the covariance function itself. For $c$ fixed to a large enough value, Figure \ref{fig1_Post_J} shows clearly how $m$ affects the accuracy on the approximation and the non-linearity of the estimated function, in the sense that fewer basis functions inaccurately imply larger lengthscales and consequently more linear functional forms.
The higher the "wigglyness" of the function to be estimated, the more basis functions will be required. If $m$ fixed to a large enough value, Figure \ref{fig2_Post_L} shows that $c$ mainly affects the approximation near the boundaries as well as covariances at long distances.


\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig1_Post_J.pdf}}
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig1_Cov_J.pdf}}
\subfigure{\includegraphics[scale=0.40, trim = 27mm 32mm 122mm 10mm, clip]{ch5_fig1_legend.pdf}}
\caption{Mean posterior predictive functions (left) and covariance functions (right) of both the regular GP model (dashed red line) and the HSGP model for different number of basis functions $m$, with the boundary factor fixed to a large enough value. Notice that the dashed red line of the regular GP can hardly be seen in the plots because it is under the blue lines.}
  \label{fig1_Post_J}
\end{figure}

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig2_Post_L.pdf}}
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig2_Cov_L.pdf}}
\subfigure{\includegraphics[scale=0.40, trim = 27mm 32mm 120mm 0mm, clip]{ch5_fig2_legend.pdf}}
\caption{Mean posterior predictive functions (left) and covariance functions (right) of both the regular GP model (dashed red line) and the HSGP model for different values of the boundary factor $c$, with a large enough fixed number of basis functions. Notice that the dashed red line of the regular GP can hardly be seen in the plots because it is under the blue lines.}
  \label{fig2_Post_L}
\end{figure}


Next, we will focus on analyzing the interaction effects between these $m$ and $c$ on the performance of the approximation. The lengthscale and marginal variance of the kernel will no longer be fixed but rather estimated in both regular GP and HSGP models. Figure \ref{fig3_Post_part1} shows the functional posterior predictions and the covariance function obtained after fitting the data, for varying $m$ and $c$. Figure \ref{fig4_MSE_vs_J} shows the root mean square error (RMSE) of the HSGP models, computed against the regular GP model. Figure \ref{fig5_lscale_vs_J} shows the estimated lengthscale and marginal variance for the regular GP model and the HSGP models. Looking at the RMSEs in Figure \ref{fig4_MSE_vs_J}, we can conclude that the optimal choice in terms of precision and computations would be $m = 15$ basis functions and a boundary factor between $c = 1.5$ and $c = 2.5$. Further, the choice of $m = 10$ and $c = 1.5$ could still be an accurate enough choice. We may also come to the same conclusion by looking at the posterior predictions and covariance function plots in Figure \ref{fig3_Post_part1}. From these results, some general conclusions may be drawn:

\begin{itemize}
\item As $c$ increases, $m$ has to increase as well (and vice versa).
\item There exists a minimum $c$ below which a close approximation will never be achieved regardless of $m$.
\end{itemize}


\begin{figure}
\begin{center}
\begin{tabular}{| c | c | c | c |}
\hline
c  = 1.05 & \includegraphics[scale=0.215, trim = 0mm 14mm 0mm 14mm, clip]{ch5_fig3_Post_part1.pdf} & \includegraphics[scale=0.215, trim = 0mm 14mm 0mm 14mm, clip]{ch5_fig3_Cov_part1.pdf} & \multirow{32}{1.3cm}{\includegraphics[scale=0.35, trim = 28mm 30mm 100mm 30mm, clip]{ch5_fig3_legend.pdf}}\\ 
\cline{1-3}
c  = 1.1 & \includegraphics[scale=0.215, trim = 0mm 14mm 0mm 14mm, clip]{ch5_fig3_Post_part2.pdf} & \includegraphics[scale=0.215, trim = 0mm 14mm 0mm 14mm, clip]{ch5_fig3_Cov_part2.pdf} &\\
\cline{1-3}
c  = 1.2 & \includegraphics[scale=0.215, trim = 0mm 14mm 0mm 14mm, clip]{ch5_fig3_Post_part3.pdf} & \includegraphics[scale=0.215, trim = 0mm 14mm 0mm 14mm, clip]{ch5_fig3_Cov_part3.pdf} &\\
\cline{1-3}
c  = 1.5 & \includegraphics[scale=0.215, trim = 0mm 14mm 0mm 14mm, clip]{ch5_fig3_Post_part4.pdf} & \includegraphics[scale=0.215, trim = 0mm 14mm 0mm 14mm, clip]{ch5_fig3_Cov_part4.pdf} & \\
\cline{1-3}
c  = 2 & \includegraphics[scale=0.215, trim = 0mm 14mm 0mm 14mm, clip]{ch5_fig3_Post_part5.pdf} &  \includegraphics[scale=0.215, trim = 0mm 14mm 0mm 14mm, clip]{ch5_fig3_Cov_part5.pdf} & \\
\cline{1-3}
c  = 2.5 & \includegraphics[scale=0.215, trim = 0mm 4mm 0mm 14mm, clip]{ch5_fig3_Post_part6.pdf} & \includegraphics[scale=0.215, trim = 0mm 4mm 0mm 14mm, clip]{ch5_fig3_Cov_part6.pdf} &\\
\hline
\end{tabular}
\end{center}
\caption{Mean posterior predictive functions (left) and covariance functions (right) of both the regular GP model and the HSGP model for different number of basis functions $m$ and for different values of the boundary factor $c$. Notice that the dashed red line of the regular GP can hardly be seen in the some of the plots because it is under the blue lines.}
  \label{fig3_Post_part1}
\end{figure}


\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 0mm 0mm, clip]{ch5_fig4_MSE_vs_J.pdf}}
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig4_MSE_vs_c.pdf}}
\caption{Root mean square error (RMSE) of the proposed HSGP models computed against the regular GP model. (left) RMSE versus the number of basis functions $m$ and for different values of the boundary factor $c$. (right) RMSE versus the boundary factor $c$ and for different values of the number of basis functions $m$. }
  \label{fig4_MSE_vs_J}
\end{figure}

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig5_lscale_vs_J.pdf}}
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig5_magnitud_vs_J.pdf}}
\subfigure{\includegraphics[scale=0.40, trim = 28mm 28mm 119mm 0mm, clip]{ch5_fig5_legend.pdf}}
\caption{Estimated lengthscale (left) and marginal variance (right) parameters of both regular GP and HSGP models, plotted versus the number of basis functions $m$ and for different values of the boundary factor $c$.}
  \label{fig5_lscale_vs_J}
\end{figure}


Additionally, there is a clear relation of the number of basis functions $m$ and the boundary factor $c$ with the lengthscale $\ell$ of the approximated function. Figures \ref{fig6_lscale_vs_J_vs_c} and \ref{fig6_lscale_vs_J_vs_c_Matern} depicts how these three factors interact with each other in relation to a close approximation of the HSGP model, in the cases of a GP with square exponential covariance function and Mat\'ern ($\nu$=3/2) covariance function, respectively, and a single input dimension. More precisely, for a given GP model (with a square exponential covariance function) with lengthscale $\ell$ and given a boundary factor $c$, Figure \ref{fig6_lscale_vs_J_vs_c} shows the minimum $m$ required to achieve a close approximation in terms of satisfying equation (\ref{eq_diff_covs}). Similarly for Figure \ref{fig6_lscale_vs_J_vs_c_Matern} in the case of a Mat\'ern ($\nu$=3/2) covariance function. We have considered an approximation to be a close enough when the difference between densities of the approximate covariance function and the exact covariance function, $\varepsilon$ in equation (\ref{eq_diff_covs}), is below 1$\%$ of the density of the exact covariance function:
%
\begin{eqnarray*}
 \frac{\varepsilon}{\int k(\tau) \, d\tau} < 0.01.
\end{eqnarray*}

\noindent Alternatively, these figures could be understood as providing the minimum $c$ that we should use for given $\ell$ and $m$. Of course, we may also read it as providing the minimum $\ell$ that can be closely approximated given $m$ and $c$. We obtain the following main conclusions:

\begin{itemize}
\item As $\ell$ increases, $c$ and $m$ required for a close enough approximation decrease.
\item The lower $c$, the smaller $m$ can and $\ell$ must be to achieve a close approximation.
\item For a given $\ell$ there exist a minimum $c$ under which a close approximation is never going to be achieved regardless of $m$. This fact can be appreciated in the Figure as the contour lines which represent $c$ have an end in function of $\ell$ (Valid $c$ are restricted in function of $\ell$).
\end{itemize}

As stated above, Figures \ref{fig6_lscale_vs_J_vs_c} and \ref{fig6_lscale_vs_J_vs_c_Matern} provide the minimum lengthscale that can be closely approximated given $m$ and $c$. This information serves as a powerful diagnostic tool in determining if the obtained accuracy is acceptable. As the lenghscale $\ell$ controls the "wigglyness" of
the functional relationship, it strongly influences the difficulty of obtaining accurate
inference about the function from the data. Basically, if the lengthscale estimate is accurate, 
we can expect the HSGP approximation to be accurate as well. Thus, having obtained an estimate $\hat{\ell}$ of $\ell$ from the HSGP model based on prespecified $m$ and $c$, we can check whether or not $\hat{\ell}$ exceeds the minimum lengthscale provided in Figure \ref{fig6_lscale_vs_J_vs_c}. If $\hat{\ell}$ exceeds this recommended minimum lengthscale, the approximation should be close enough. If, however, it does not exceed it, the approximation may be inaccurate and $m$ should be increased or $c$ decreased. We may also use this diagnostic in a iterative procedure.
Starting from some initial guess of $\ell$, we can choose initial values for $m$ and $c$ and fit an HSGP model, then check the approximation accuracy, and, if not accurate enough because the estimated $\hat{\ell}$ is below the minimum lengthscale provided by Figure \ref{fig6_lscale_vs_J_vs_c}, repeat the process while increasing $m$ or decreasing $c$. Note that, as commented before, $c$ can not be decreased as much as desired because it is restricted to the lengthscale.

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.40, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig6_lscale_vs_J_vs_c.pdf}}
\hspace{3mm}
\subfigure{\includegraphics[scale=0.40, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig6_lscale_vs_J_vs_c_zoomin.pdf}}
\caption{Relation among the minimum number of basis functions $m$, the boundary factor $c$ ($c = \frac{L}{S}$) and the lengthscale normalized by the half-range of the data ($\frac{\ell}{S}$), in the case of a square exponential covariance function. The right-side plot is a zoom in of the left-side plot.}
  \label{fig6_lscale_vs_J_vs_c}
\end{figure}

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.40, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig6_lscale_vs_J_vs_c_Matern.pdf}}
\hspace{3mm}
\subfigure{\includegraphics[scale=0.40, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig6_lscale_vs_J_vs_c_zoomin_Matern.pdf}}
\caption{Relation among the minimum number of basis functions $m$, the boundary factor $c$ ($c = \frac{L}{S}$) and the lengthscale normalized by the half-range of the data ($\frac{\ell}{S}$), in the case of a Mat\`ern($\nu$=3/2) covariance function. The right-side plot is a zoom in of the left-side plot.}
  \label{fig6_lscale_vs_J_vs_c_Matern}
\end{figure}


If we look back to the conclusions drawn from Figures \ref{fig4_MSE_vs_J} and \ref{fig5_lscale_vs_J}, where $m = 10$ basis functions and a boundary factor of $c = 1.5$ were enough to closely approximate a function with $\ell = 0.3$, we can recognize that these conclusions also matches those obtained from Figure \ref{fig6_lscale_vs_J_vs_c}.

Figures \ref{fig6_lscale_vs_J_vs_c} and \ref{fig6_lscale_vs_J_vs_c_Matern} were build for a GP with a unidimensional covariance function, which result in a surface depending on three variables, $m$, $c$ and $\ell$. An equivalent figure for a GP model with a two-dimensional covariance function would result in a surface depending on four variables, $m$, $c$, $\ell_1$ and $\ell_2$, which can not be graphically represented. More precisely, in the multi-dimensional case, whether the approximation is close enough might depend only on the ratio between wiggliness in every dimensions. For instance, in the two-dimensional case it would depend on the ratio between $\ell_1$ and $\ell_2$ and could be graphically represented. Future research will focus on building useful graphs or analytical models that provide these relations in multi-dimensional cases. However, as an approximation, we can use the unidimensional GP conclusions in Figures \ref{fig6_lscale_vs_J_vs_c} and \ref{fig6_lscale_vs_J_vs_c_Matern} to check the accuracy by analyze individually the different dimensions of a multidimensional GP model.

\subsection{Comparing lengthscale estimates}

In this example, we make a comparison of the lengthscale estimates
obtained from the regular GP and HSGP models. We also have a look at those recommended minimum lengthscales provided by Figure \ref{fig6_lscale_vs_J_vs_c}.

For this analysis, we will use various datasets consisting of noisy draws from a GP prior model with a squared exponential covariance function and varying lengthscale values. Different values of the number of basis functions $m$ are used when estimating the HSGP models, and the boundary factor $c$ is set to a valid and optimum value in every case. 

Figure \ref{fig7_posterior_varing_lscale_part1} shows the posterior predictions of both regular GP and HSGP models fitted to those datasets. The lengthscale estimates as obtained by regular GP and HSGP models are depicted in Figure \ref{fig8_Tlscale_vs_Elscale}. As noted previously, an accurate estimate of the lengthscale can be a good indicator of a close approximation of the HSGP model to the regular GP model. Further, Figure \ref{fig9_MSE_varing_lscale} shows the root mean square error (RMSE) of the HSGP models, computed against the regular GP models, as a function of the lengthscale and number of basis functions.

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.4, trim = 10mm 100mm 5mm 30mm, clip]{ch5_fig7_legend.pdf}}\\
\vspace{-3mm}
\subfigure{\includegraphics[scale=0.30, trim = 1mm 25.5mm 10mm 19mm, clip]{ch5_fig7_posterior_varing_lscale_part1.pdf}}
\subfigure{\includegraphics[scale=0.30, trim = 21mm 25.5mm 10mm 19mm, clip]{ch5_fig7_posterior_varing_lscale_part2.pdf}}
\subfigure{\includegraphics[scale=0.30, trim = 21mm 25.5mm 10mm 19mm, clip]{ch5_fig7_posterior_varing_lscale_part3.pdf}}\\
\vspace{-3mm}
\subfigure{\includegraphics[scale=0.30, trim = 1mm 25.5mm 10mm 19mm, clip]{ch5_fig7_posterior_varing_lscale_part4.pdf}}
\subfigure{\includegraphics[scale=0.30, trim = 21mm 25.5mm 10mm 19mm, clip]{ch5_fig7_posterior_varing_lscale_part5.pdf}}
\subfigure{\includegraphics[scale=0.30, trim = 21mm 25.5mm 10mm 19mm, clip]{ch5_fig7_posterior_varing_lscale_part6.pdf}}\\
\vspace{-3mm}
\subfigure{\includegraphics[scale=0.30, trim = 1mm 5mm 10mm 19mm, clip]{ch5_fig7_posterior_varing_lscale_part7.pdf}}
\subfigure{\includegraphics[scale=0.30, trim = 21mm 5mm 10mm 19mm, clip]{ch5_fig7_posterior_varing_lscale_part8.pdf}}
\subfigure{\includegraphics[scale=0.30, trim = 21mm 5mm 10mm 19mm, clip]{ch5_fig7_posterior_varing_lscale_part9.pdf}}\\
\vspace{-3mm}
\caption{Mean posterior predictions of both regular GP and HSGP models, fitted over various datasets drawn from square exponential GP models with different characteristic lengthscales ($\ell$) and same marginal variance ($\alpha$) as the data-generating functions (\textit{True function}). Notice that the dashed red line of the regular GP can hardly be seen in the plots because it is under the blue lines.}
  \label{fig7_posterior_varing_lscale_part1}
\end{figure}

\begin{figure}
\begin{flushleft}
\begin{tabular}{ccc}
\multicolumn{3}{c}{ \includegraphics[scale=0.40, trim = 25mm 117mm 90mm 45mm, clip]{ch5_fig8_legend.pdf}}\\
\includegraphics[scale=0.30, trim = 0mm 24mm 7mm 10mm, clip]{ch5_fig8_Tlscale_vs_Elscale_part1.pdf} & \hspace{-4mm}
\includegraphics[scale=0.30, trim = 25mm 24mm 7mm 10mm, clip]{ch5_fig8_Tlscale_vs_Elscale_part2.pdf} & \hspace{-4mm} \includegraphics[scale=0.30, trim = 25mm 24mm 7mm 10mm, clip]{ch5_fig8_Tlscale_vs_Elscale_part3.pdf} \\ 
\includegraphics[scale=0.30, trim = 0mm 0mm 7mm 10mm, clip]{ch5_fig8_Tlscale_vs_Elscale_part4.pdf} & \hspace{-4mm} \includegraphics[scale=0.30, trim = 25mm 0mm 7mm 10mm, clip]{ch5_fig8_Tlscale_vs_Elscale_part5.pdf} & \hspace{-4mm} \includegraphics[scale=0.30, trim = 25mm 0mm 7mm 10mm, clip]{ch5_fig8_Tlscale_vs_Elscale_part6.pdf}
\end{tabular}
\end{flushleft}
\caption{Data-generating functional lengthscales ($\ell$), of the various datasets illustrated in Figure \ref{fig7_posterior_varing_lscale_part1}, versus the corresponding lengthscale estimates $(\hat{\ell})$ from the regular GP and HSGP models. 95\% confident intervals of the lengthscale estimates are plotted as dot lines. The different plots represent the use of different number of basis functions $m$ in the HSGP model. The dashed black line represents the recommended minimum lengthscales provided by Figure \ref{fig6_lscale_vs_J_vs_c} that can be closely approximated by the HSGP model in every case.}
  \label{fig8_Tlscale_vs_Elscale}
\end{figure}


\begin{figure}
\centering
\includegraphics[scale=0.38]{ch5_fig9_MSE_varing_lscale.pdf}
\caption{RMSE of the HSGP models with different number of basis functions $m$, for the various datasets with different wiggly effects ($\ell$).}
  \label{fig9_MSE_varing_lscale}
\end{figure}

Comparing the accuracy of the lengthscale in Figure 
\ref{fig8_Tlscale_vs_Elscale} to the RMSE in Figure \ref{fig9_MSE_varing_lscale},
we see that they agree closely with each other for medium lengthscales.
That is, a good estimation of the lengthscale implies a
small RMSE. This is no longer true for very small or large lengthscales.
In small lengthscales, even very small inaccuracies 
may have a strong influence on the posteriors predictions and thus on the RMSE.
In large lengthscales, larger inaccuracies change the posterior predictions
only little and may thus not yield large RMSEs.
The dashed black line in Figure \ref{fig8_Tlscale_vs_Elscale} represents the 
minimum lengthscale that can be closely approximated under the given condition, according to the results presented in Figure \ref{fig6_lscale_vs_J_vs_c}. 
We observe that whenever the estimated lengthscale exceeds the minimially
estimable lengthscale, the RMSE of the posterior predictions is small
(see Figure \ref{fig9_MSE_varing_lscale}).
Conversely, when the estimated lengthscale is smaller than the minimally
estimable one, the RMSE becomes very large.


\section{Univariate case studies}\label{ch5_sec_univariate_cases}

\subsection{Simulated data}\label{ch5_sec_univariate_simu}

This example consists of a simulated dataset with $n=250$ ($i=1,\dots,n$) single draws from a Gaussian process prior with a Mat{\'e}rn($\nu$=3/2) covariance function and hyperparameters marginal variance $\alpha=1$ and lengthscale $\ell=0.15$, with corresponding inputs values $\bm{x}=(x_1\dots,x_n)$ with $x_i \in [-1,1] \subset {\rm I\!R}$. To form the final noisy dataset $\bm{y}$, Gaussian noise $\sigma=0.2$ was added to the GP draws.

The regular GP model for fitting this simulated dataset $\bm{y}$ can be written as follows,
%
\begin{eqnarray*}\label{ch5_eq_latentgp_simudata1}
\begin{split}
\bm{y} &= \bm{f} + \bm{\epsilon} \\
\bm{\epsilon} &\sim \mathcal{N}(0, \sigma^2  I) \\
f(x) &\sim \mathcal{GP}(0, k(x, x', \theta)),
\end{split}
\end{eqnarray*}

\noindent where $I$ represents the identity matrix and $\bm{f}=\{f(x_i)\}_{i=1}^n$ represents the underlying function values to the noisy data. The previous formulation corresponds to the latent form of a GP model. The function $f:{\rm I\!R} \to {\rm I\!R}$ is a GP prior with a Mat{\'e}rn($\nu$=3/2) covariance function $k$. Saying that the function $f(\cdot)$ follows a GP model is equivalent to say that $\bm{f}$ is multivariate Gaussian distributed with covariance matrix $K$, where $K_{ij}=k(x_i,x_j,\theta)$, with $i,j=1,\dots,n$.
 
A more computationally efficient formulation of a GP model with Gaussian likelihood, and for probabilistic inference using sampling methods such as HMC, would be its marginalized form,
%
\begin{equation*}\label{ch5_eq_marginalizedgp_simudata1}
\bm{y} \sim \mathcal{N}(0, K + \sigma^2 I ),
\end{equation*}

\noindent where the function values $\bm{f}$ have been integrated out, yielding a lower-dimensional parameter space over which to do inference, reducing the time of computation and improving the sampling and the effective number of samples.

In the HSGP model, the latent function values $f(x)$ are approximated as in equation (\ref{eq_approxf}), with the Mat{\'e}rn($\nu$=3/2) spectral density $S$ as in equation (\ref{eq_specdens_32}), and eigenvalues $\lambda_j$ and eigenfunctions $\phi_j$ as in equations (\ref{eq_eigenvalue}) and (\ref{eq_eigenfunction}), respectively. 

In order to do model comparison, in addition to the regular GP model and HSGP model, a spline-based model is also fitted using the thin plate regression spline approach in \cite{wood2003thin} and implemented in the R-package \textit{mgcv} \citep{wood2015package}. A Bayesian approach is used to fit this spline model using the R-package \textit{brms} \citep{burkner2017brms}.

Figure \ref{ch5_fig10_Posteriors_exI} shows the posteriors predictive distributions of the three models, the regular GP, the HSGP with $m=80$ basis functions and boundary factor $c=1.2$ ($L=c\cdot 1= 1.2$; see equation (\ref{eq_boundary})), and the spline model with 80 knots. The true data-generative function and the noisy observations are also plotted. The sample observations are plotted as circles and the out-of-sample or test data, which have not been taking part on training the models, are plotted as crosses. The test data located at the extremes of the plot are used for assessing model extrapolation, and the test data located in the middle are used for assessing model interpolation. The posteriors of the three models, regular GP, HSGP and spline, are pretty similar within the interpolation input space. However, when extrapolating the spline model solution clearly differs from the regular GP and HSGP models as well as the actual observations. 

\begin{figure}
\centering
\includegraphics[width=\textwidth]{ch5_fig10_Posteriors_exI.pdf}
\caption{Posterior predictive means of the proposed HSGP model, the regular GP model, and the spline model. 95\% credible intervals are plotted as dashed lines.}
  \label{ch5_fig10_Posteriors_exI}
\end{figure}

In order to assess the performance of the models as a function of the number of basis functions and number of knots, different models with different number of basis functions for the HSGP model, and different number of knots for the spline model, have been fitted. Figure \ref{ch5_fig11_MSE_exI_inter} shows the standardized root mean squared error (SRMSE) for interpolation and extrapolating data as a function of the number of basis functions and knots. The SRMSE is computed against the data-generating model. From Figures \ref{ch5_fig10_Posteriors_exI} and \ref{ch5_fig11_MSE_exI_inter}, it can be seen a close approximation of the HSGP model to the regular GP model for interpolating and extrapolating data. However, the spline model does not extrapolate data properly. Both models show roughly similar interpolating performance. 

Figure \ref{ch5_fig11_time_exI} shows computational times, in seconds per iteration (iteration of the HMC sampling method), as a function of the number of basis functions $m$, for the HSGP model, and knots, for the spline model. The computational time is represented in the y-axis of the figure, which is in a logarithmic scale. The HSGP model is on average roughly 400 times faster than the regular GP, in the particular case of applying over this dataset.

The Stan model codes for the exact GP, the approximate GP and the spline models of this case study can be found through the following link:
%
\begin{lstlisting}[breaklines]
https://github.com/gabriuma/Doctoral_thesis/tree/master/Case-study-I_1D-Simulated-data
\end{lstlisting}

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig11_MSE_exI_inter.pdf}}
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig11_MSE_exI_extra.pdf}}
\subfigure{\includegraphics[scale=0.40, trim = 29mm 55mm 110mm 0mm, clip]{ch5_fig11_MSE_exI_legend.pdf}}
\caption{Standardized root mean square error (SRMSE) of the different methods against the data-generating function. SRMSE for interpolation (left) and SRMSE for extrapolation (right). The standard deviation of the mean of the SRMSE is plotted as dashed lines.}
  \label{ch5_fig11_MSE_exI_inter}
\end{figure}

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig11_time_exI.pdf}}
\caption{Computational time (y-axis), in seconds per iteration (iteration of the HMC sampling method), as a function of the number of basis functions $m$, for the HSGP model, and knots, for the spline model. The y-axis is on a logarithmic scale. The standard deviation of the computational time is plotted as dashed lines.}
  \label{ch5_fig11_time_exI}
\end{figure}


\subsection{Birthday data}\label{ch5_sec_birthday}
This example is an analysis of patterns in birthday frequencies in a dataset containing records of all births in the United States on each day during the period 1969–1988. The model decomposes the number of births along all the period in longer-term trend effects, patterns during the year, day-of-week effects, and special days effects. The special days effects cover patterns such as possible fewer births on Halloween, Christmas or new year, and excess of births on Valentine’s Day or the days after Christmas (due, presumably, to choices involved in scheduled deliveries, along with decisions of whether to induce a birth for health reasons). This analysis was originally addressed in \cite{gelman2013bayesian}. The total number of days within the period is $T=7305$ ($t=1,\dots,T$), then a regular GP model is unfeasible to be fitted on this dataset as we know inference scales $O(T^3)$ in covariance matrix inversion. 
Therefore, an approximate approach has to be used to fit a GP model on this data. We will use the HSGP model developed in this Chapter, as well as the low-rank GP model with a periodic covariance function introduced in Section \ref{ch5_sec_periodic} which is based on expanding the periodic covariance function into a series of stochastic resonators \citep{solin2014explicit}.

Let's denote $y_t$ as the number of births of day $t$. The observational model is a normal model with parameters the mean function $\mu(t)$ and noise variance $\sigma^2$,
%
\begin{equation*}
y_{t} \sim \mathcal{N}(\mu(t),\sigma^2).
\end{equation*}

\noindent The mean function $\mu(t)$ will be defined as an additive model in the form: 
%
\begin{eqnarray} \label{ch5_eq_mean_brithday}
\mu(t) = f_1(t) + f_2(t) + f_3(t) + f_4(t).
\end{eqnarray}

The component $f_1(t)$ represents the long-term trends modeled by a GP with squared exponential covariance function,
%
\begin{eqnarray*}
f_1(t) \sim \mathcal{GP}(0,k_1), \hspace{5mm} k_1(t,t') = \sigma_1^2 \exp\bigg(-\frac{1}{2} \frac{(t-t')^2}{\ell_1^2}\bigg), 
\end{eqnarray*}

\noindent which means the function values $\bm{f}_1=\{f_1(t)\}_{t=1}^T$ are multivariate Gaussian distributed with covariance matrix $K_1$, where $K_{1_{t,s}}=k_1(t,s)$, with $t,s=1,\dots,T$.

The component $f_2(t)$ represents the yearly smooth seasonal pattern, using a periodic squared exponential covariance function (with period 365.25 to match the average length of the year) in a GP model,
%
\begin{eqnarray*}
f_2(t) \sim \mathcal{GP}(0,k_2), \hspace{5mm} k_2(t,t') = \sigma_2^2 \exp\bigg(-\frac{2\text{sin}^2(\pi(t-t')/365.25}{\ell_2^2}\bigg), 
\end{eqnarray*}

\noindent which means the function values $\bm{f}_2=\{f_2(t)\}_{t=1}^T$ are multivariate Gaussian distributed with covariance matrix $K_2$, where $K_{2_{t,s}}=k_2(t,s)$, with $t,s=1,\dots,T$.

The component $f_3(t)$ represents the weekly smooth pattern using a periodic squared exponential covariance function (with period 7 of length of the week) in a GP model,
%
\begin{eqnarray*}
f_3(t) \sim \mathcal{GP}(0,k_3), \hspace{5mm} k_3(t,t') = \sigma_3^2 \exp\bigg(-\frac{2\text{sin}^2(\pi(t-t')/7}{\ell_3^2}\bigg), 
\end{eqnarray*}

\noindent which means the function values $\bm{f}_3=\{f_3(t)\}_{t=1}^T$ are multivariate Gaussian distributed with covariance matrix $K_3$, where $K_{3_{t,s}}=k_3(t,s)$, with $t,s=1,\dots,T$.

The component $f_4(t)$ represents the special days effects, modeled as a horse-shoe prior model \citep{piironen2017sparsity}:
%
\begin{eqnarray*}
f_4(t) \sim \mathcal{N}(0,\lambda^2_t \tau^2), \hspace{5mm} \lambda^2_t \sim \mathcal{C}^{+}(0,1).
\end{eqnarray*}

\noindent A horse-shoe prior allows for sparse distributed effects. Its global parameter $\tau$ pulls all the weights (effects) globally towards zero, while the thick half-Cauchy tails for the local scales $\lambda_t$ allow some of the weights to escape the shrinkage. Different levels of sparsity can be accommodated by changing the value of $\tau$: with large $\tau$ all the variables have very diffuse priors with very little shrinkage towards zero, but letting $\tau \rightarrow 0$ will shrink all the weights $f_4(t)$ to zero \citep{piironen2016hyperprior}. 
%More details about the specified priors for $\tau$ and $\lambda_t$ have been included in Appendix \ref{app_horse_shoe}.

GP priors have been defined over the components $f_1(t)$, $f_2(t)$ and $f_3(t)$. Then, low-rank representations of the GP priors have to be used in the modeling and inference. The component $f_1(t)$ will be approximated using the HSGP model. Thus, the function values $f_1(t)$ are approximated as in equation (\ref{eq_approxf}), with the squared exponential spectral density $S$ as in equation (\ref{eq_specdens_inf}), and eigenvalues $\lambda_j$  and eigenfunctions $\phi_j$ as in equations (\ref{eq_eigenvalue}) and (\ref{eq_eigenfunction}). 

The year effects $f_2(t)$ and week effects $f_3(t)$, as they use a periodic covariance function, they do no fit under the main framework of the HSGP approximation covered in this chapter. However, they do have a representation based on expanding periodic covariance functions into a series of stochastic resonators (Appendix \ref{ch5_sec_periodic}). Thus, the functions $f_2(t)$ and $f_3(t)$ are approximated as in equation (\ref{ch5_eq_f_period}), with variance coefficients $\tilde{q}_j^2$ as in equation (\ref{ch5_eq_q_2}).

For the component $f_1(t)$, $m=30$ basis functions and a boundary factor $c=1.5$ were used. The lengthscale estimate $\hat{\ell}_1$, for this component, normalized by half of the range of the input $x_1$, is bigger than the minimum lengthscale reported by Figure \ref{fig6_lscale_vs_J_vs_c} as a function of $m$ and $c$. Which means that the used number of basis functions and boundary factor are suitable values for modeling accurately the input effects. 

For the components $f_2(t)$ and $f_3(t)$, $J=10$ cosine terms were used. The  lengthscales estimates $\hat{\ell}_2$ and $\hat{\ell}_3$, for the GP components $f_2(t)$ and $f_3(t)$, respectively, are bigger than the minimum lengthscale reported by Figure \ref{ch5_fig28_m_lscale_periodic} as function of the number of cosine terms $J$, which means that the approximations are accurate enough.

Figure \ref{ch5_fig27_posteriors_birthday} shows the posterior means of the long-term trend $f_1(t)$ and year patterns $f_2(t)$ for the whole period, jointly with the observed data. Figure \ref{ch5_fig27_posteriors_oneyear_birthday} show the process for one year (1972) only. In this figure, the special days effects $f_4(t)$ in the year can be clearly represented. The posterior means of the the function $\mu(t)$ and the components $f_1(t)$ (long-term trend) and $f_2(t)$ (year pattern) are also plotted in this Figure \ref{ch5_fig27_posteriors_oneyear_birthday}. Figure \ref{ch5_fig27_posteriors_onemonth_birthday} show the process in the month of January of 1972 only, where the week pattern $f_3(t)$ can be clearly represented. The mean of the the function $\mu(t)$ and components $f_1(t)$ (long-term trend), $f_2(t)$ (year pattern) and $f_4(t)$ (special-days effects) are also plotted in this Figure \ref{ch5_fig27_posteriors_onemonth_birthday}. 

The Stan model code for the approximate GP model of this case study can be found in:
%
\begin{lstlisting}[breaklines]
https://github.com/gabriuma/Doctoral_thesis/tree/master/Case-study-II_Birthday-data
\end{lstlisting}

\begin{figure}
\centering
\subfigure{\includegraphics[width=\textwidth, trim = 0mm 0mm 0mm 0mm, clip]{ch5_fig27_posteriors_birthday.pdf}}
\caption{Posterior means of the long-term trend ($f_1(\cdot)$) and year effects pattern ($f_2(\cdot)$) for the whole series. }
  \label{ch5_fig27_posteriors_birthday}
\end{figure}

\begin{figure}
\centering
\subfigure{\includegraphics[width=\textwidth, trim = 0mm 0mm 0mm 0mm, clip]{ch5_fig27_posteriors_oneyear_birthday.pdf}}
\caption{Posterior means of the function $\mu(\cdot)$ for the year 1972 of the series. The special days effects pattern ($f_4(\cdot)$) in the year is also represented, as well as the long-term trend ($f_1(\cdot)$) and year effects pattern ($f_2(\cdot)$). }
  \label{ch5_fig27_posteriors_oneyear_birthday}
\end{figure}

\begin{figure}
\centering
\subfigure{\includegraphics[width=\textwidth, trim = 0mm 0mm 0mm 0mm, clip]{ch5_fig27_posteriors_onemonth_birthday.pdf}}
\caption{Posterior means of the function $\mu(\cdot)$ for the month of January of 1972. The week effects pattern ($f_3(\cdot)$) in the month is also represented, as well as the long-term trend ($f_1(\cdot)$), year effects pattern ($f_2(\cdot)$) and special days effects pattern ($f_4(\cdot)$). }
  \label{ch5_fig27_posteriors_onemonth_birthday}
\end{figure}

 
\section{Multivariate case studies}\label{ch5_sec_multivariate_cases}

\subsection{Leukemia data}\label{ch5_sec_bf_caseVI}
The next example presents a survival analysis in acute myeloid leukemia (AML) in adults, with data recorded between 1982 and 1998 in the North West Leukemia Register in the United Kingdom. The data set consist in survival times $t_i$ and censoring indicator $z_i$ (0 for observed and 1 for censored) for $n=1043$ cases ($i=1,\dots,n$). Some 16\% of cases were censored. Predictors are \textit{age} ($\bm{x}_1$), \textit{sex} ($\bm{x}_2$), \textit{white blood cell} (WBC) ($\bm{x}_3$) count at diagnosis with 1 unit = $50\times109/L$, and the \textit{Townsend deprivation index} (TDI) ($\bm{x}_4$) which is a measure of deprivation for district of residence. We denote the matrix $X=[\bm{x}_1\; \bm{x}_2 \; \bm{x}_3 \; \bm{x}_4]^\top \in {\rm I\!R}^{n\times 4}$ which contains the predictors.

As the WBC measurements were strictly positive and highly skewed, we fit the model to its logarithm. Continuous predictors were normalized to have zero mean and unit standard deviation. %Survival time was normalized to have zero mean for the logarithm of time. 
We assume a log Gaussian observation model for the observed survival time, $t_i$, with a function of the predictors, $f(X_i)$, as the location parameter, and $\sigma$ as the Gaussian noise: 
%
\begin{equation*}
p(t_i)= \mathrm{LogNormal}(t_i|f(X_i),\sigma^2)
\end{equation*}

\noindent with $X_i=\{x_{i1},x_{i2},x_{i3},x_{i4}\} \in {\rm I\!R}^4$ and $f:{\rm I\!R} \to {\rm I\!R}$.

As we do not have a model for the censoring process, we do not have a full observation model, and the observational model for the censored data $t_i$ is assumed to be the complementary cumulative normal probability distribution:
%
\begin{equation*}
p(y_i > t_i)= \int_{t_i}^{\infty} \mathrm{LogNormal}(y_i|f(X_i),\sigma^2) dy_i=  1 - \Phi \left( \frac{\mathrm{log}(y_i)-f(X_i)}{\sigma} \right),
\end{equation*}

\noindent where $y_i$ denotes the uncensored time.

The latent function $f(\cdot)$ is modeled as a Gaussian process, centered in a linear model of the predictors $X$, and with a squared exponential covariance function $k$ depending on predictors $X$ and hyperparameters $\theta=(\alpha,\ell)$,
%
\begin{eqnarray*} 
f(\bm{x}) \sim \mathcal{GP}(c + \bm{\beta}\bm{x}, k(\bm{x},\bm{x}', \theta)),
\end{eqnarray*}

\noindent where $c$ and $\bm{\beta}$ are the intercept and vector of coefficients, respectively, of the linear model.  Saying that the function $f(\cdot)$ follows a GP model is equivalent to say that $\bm{f}$ are multivariate Gaussian distributed with mean function $\mu(\cdot)$ and covariance matrix $K$, where $\mu(\bm{x}_i)=c + \bm{\beta}\bm{x}_i$ and $K_{rs}=k(\bm{x}_r,\bm{x}_s,\theta)$, with $r,s=1,\dots,n$.  The hyperparameters $\alpha$ and $\ell$ represent the marginal variance and lengthscale, respectively, of the GP process. Notice that a scalar lengthscale is considered in the multivariate covariance function.


Due to the predictor \textit{sex} ($\bm{x}_2$) is a categorical variable (1 for female and 2 for male), we can outline a multilevel model for the GP function, in a similar way like categorical effects are treated in linear models. The relative contribution of a GP function given one of the levels of the predictor (equation \ref{ch5_eq_gpml2}) to a general mean GP function (equation \ref{ch5_eq_gpml1}) is defined. For the other level of the predictor, the GP function effects are set to zero. This multilevel construction is depicted as follows:
%
\begin{eqnarray} 
&f(\bm{x}) \sim \mathcal{GP}(c + \bm{\beta}\bm{x}, k(\bm{x},\bm{x}', \theta_1)) \label{ch5_eq_gpml1} \\
&g(\bm{x}) \sim \mathcal{GP}(0, k(\bm{x},\bm{x}', \theta_2|\bm{x}_2=2)) \label{ch5_eq_gpml2} \\ 
&f(\bm{x}|\bm{x}_2=2) = f(X|\bm{x}_2=2) + g(\bm{x}) \nonumber
\end{eqnarray}

\begin{figure}
\centering
\includegraphics[scale=0.70, trim = 0mm 0mm 0mm 0mm, clip]{ch5_fig21_posteriors_leukemia_02.pdf}
\caption{Expected lifetime conditional comparison for each predictor with other predictors fixed to their mean values. The thick line in each graph is the posterior mean estimated using a HSGP model, and the thin lines represent pointwise 95\% credible intervals.}
  \label{ch5_fig21_posteriors_leukemia}
\end{figure}

\noindent In the previous equations, $\theta_1$ contains the hyperparameters $\alpha_1$ and $\ell_1$ which are the marginal variance and lengthscale, respectively, of the general mean GP function, and $\theta_2$ contains the hyperparameters $\alpha_2$ and $\ell_2$ which are the marginal variance and lengthscale, respectively, of the GP function restricted to the male sex ($\bm{x}_2=2$).

Using the HSGP approximation, the functions $f(\bm{x})$ and $g(\bm{x})$ are approximated as in equation (\ref{eq_approxf_multi}), with the $D$-dimensional (with a scalar lengthscale) squared exponential spectral density $S$ as in equation (\ref{eq_specdens_inf}), and the multivariate eigenfunctions $\phi_j$ and the $D$-vector of eigenvalues $\bm{\lambda}_j$ as in equations (\ref{eq_eigenfunction_multi}) and  (\ref{eq_eigenvalue_multi}), respectively.

Figure \ref{ch5_fig21_posteriors_leukemia} shows estimated conditional comparison of each predictor with all others fixed to their mean values. These posterior estimates correspond to the HSGP model with $m=10$ basis functions and $c=3$ boundary factor. The model has found smooth non-linear patterns and the right bottom subplot also shows that the conditional comparison associated with WBC has an interaction with TDI.


Figure \ref{ch5_fig22_elpd_leukemia} shows the expected log predictive density (ELPD; see \cite{vehtari_2012}) and time of computation as function of the number of univariate basis functions $m$ and boundary factor $c$. As the functions are smooth, a few number of basis functions and a large boundary factor are required to obtain a good approximation (Figure \ref{ch5_fig22_elpd_leukemia}-left); Small boundary factors are not allowed when large lengthscales, as can be seen in Figure \ref{fig6_lscale_vs_J_vs_c}. Increasing the boundary factor also significantly increases the time of computation (Figure \ref{ch5_fig22_elpd_leukemia}-right).
%
\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 0mm 0mm, clip]{ch5_fig22_elpd_leukemia.pdf}} 
\subfigure{\includegraphics[scale=0.38, trim = 0mm 0mm 10mm 0mm, clip]{ch5_fig23_time_leukemia.pdf}}
\subfigure{\includegraphics[scale=0.40, trim = 28mm 45mm 120mm 0mm, clip]{ch5_fig24_legend_leukemia.pdf}}
\caption{Expected log predictive density (ELPD) and time of computation in seconds per iteration (iteration of the HMC sampling method) as a function of the number of basis functions $m$ and boundary factor $c$.}
  \label{ch5_fig22_elpd_leukemia}
\end{figure}

The Stan model codes for the exact GP and the approximate GP models of this case study can be found in:
%
\begin{lstlisting}[breaklines]
https://github.com/gabriuma/Doctoral_thesis/tree/master/Case-study-VII_Leukemia-data
\end{lstlisting}

\section{Conclusion}\label{ch5_sec_conclusion}
The GP model entails a complexity that is computationally intractable for many applications, and this problem especially becomes severe when we want to perform inference using sampling methods. In this paper, a novel approach for a low-rank representation of GPs, originally and theoretically developed by \cite{solin2018hilbert}, has been implemented and analyzed. The method is based on a basis function approximation via Laplace eigenfunctions. The method has an attractive computational cost as this basically turns the regular GP model into a linear model, which is also an attractive property in modular probabilistic programming models. 

The formulae of the method and how it applies for all the stationary covariance functions as long as they can be represented in terms of their spectral densities has been clearly presented. The dominating cost per log density evaluation (during sampling) is $O(nm+m)$, which is a big benefit in comparison to $O(n^3)$ of a regular GP model. The design matrix is independent of hyperparameters and therefore only needs to be constructed once, at cost $O(nm)$. All dependencies on the kernel and the hyperparameters are through the prior distribution of the regression weights. The parameter posterior distribution is $m$-dimensional, where $m$ is much smaller than the number of observations $n$, which is greatly reduced in comparison to regular GPs and this makes inference faster, especially when sampling methods are used. The drawbacks of the method are the boundary conditions and scaling with respect to the number of dimensions, i.e. the number of basis functions $m$ scales exponentially with the number of dimensions.

The main contribution of this paper has been the realization of a deep analysis and diagnosis of the performance and accuracy of the approximation in relation to the key factors of the method, the number of basis functions, the boundary condition of the Laplace eigenfunctions and the non-linearity of the function to be learned. Recommendations for the values of these key factors based on the recognized relations among them have been provided, as well as useful graphs of these relations that will help the users to improve performance and save time of computation, which also serve as a powerful diagnosis tool whether the chosen values for the number of basis functions and the box size are adequate to fit to the actual data.

Furthermore, the focus has been put on showing how GPs can be easily used as modular components in probabilistic programming frameworks (i.e. Stan, WinBUGS and others) and can be used as latent functions in non-Gaussian observational models. Several illustrative examples, simulated and real datasets, of the performance of the model, where we demonstrated the applicability and the implementation of the methodology, the reduction of the computation and the improvement in sampling efficiency have been carried out.

The main drawback of this approach is that its computational complexity scales exponentially with the number of dimensions. Hence, in practice, input dimensionalities larger than 3 start to be too computationally demanding. In these cases, the proposed HSGP model can be used as low-dimensional components in an additive modeling scheme.

Future research will focus on constructing analytical models for the relationships between the key factors of the number of basis functions, the boundary factor and the lengthscale of the function, depicted in Figures \ref{fig6_lscale_vs_J_vs_c}, \ref{fig6_lscale_vs_J_vs_c_Matern} and \ref{ch5_fig28_m_lscale_periodic}, on which ultimately depend the performance of the approximation. This analytical models can be useful to automatize the diagnosis of the performance of the approximation. These relationships have been obtained, in the present study, for the unidimensional case only. So, another future research line will be focused on analyzing these relationships in the multidimensional case, building useful graphs or analytical models that encode these relationships in multidimensional cases.




\appendix

\numberwithin{equation}{section}
\numberwithin{figure}{section}

\section{Approximation of the covariance function using Hilbert space methods} \label{ch5_app_approx_covfun}


In this section, we briefly present a summary of the mathematical details of the approximation of a stationary covariance function as a series expansion of eigenvalues and eigenfunctions of the Laplacian operator. This statement is basically an extract of the work \cite{solin2018hilbert}, where the authors fully develop the mathematical theory behind the Hilbert Space approximation for stationary covariance functions.

Associated to each covariance function $k(\bm{x},\bm{x}')$ we can also define a covariance operator $\mathcal{K}$ over a function $f(\bm{x})$ as follows:
%
\begin{equation*}
\mathcal{K} f(\bm{x}) = \int k(\bm{x},\bm{x}') f(\bm{x}') d\bm{x}'.
\end{equation*} 

From the Bochner’s and Wiener-Khintchine theorem, the spectral density of a stationary covariance function $k(\bm{x},\bm{x}') = k(\bm{\tau})$, $\bm{\tau}=(\bm{x}-\bm{x}')$, is the Fourier transform of the covariance function,
%
\begin{eqnarray*}
S(\bm{w}) = \int k(\bm{\tau}) e^{-2\pi i \bm{w} \cdot \bm{\tau}}, \nonumber
\end{eqnarray*}

\noindent where $\bm{w}$ is in the frequency domain. The operator $\mathcal{K}$ will be translation invariant if the covariance function is stationary. This allows for a Fourier representation of the operator $\mathcal{K}$ as a transfer function which is the spectral density of the Gaussian process. 
%When the covariance function is homogeneous, the corresponding operator $\mathcal{K}$ will be translation invariant thus allowing for Fourier-representation as a transfer function. This transfer function is the spectral density of the Gaussian process
%%
%\begin{eqnarray*}
%S(\bm{w}) = \mathcal{F}(\mathcal{K}). \nonumber
%\end{eqnarray*}
%
%\noindent 
Thus, the spectral density $S(\bm{w})$ also gives the approximate eigenvalues of the operator $\mathcal{K}$.

In the isotropic case $S(\bm{w}) = S(||\bm{w}||)$ and assuming that the spectral density function $S(\cdot)$ is regular enough, then it can be represented as a polynomial expansion:
%
\begin{equation}\label{ch5_eq_S}
S(||\bm{w}||)=a_0+a_1||\bm{w}||^2+a_2(||\bm{w}||^2)^2+a_3(||\bm{w}||^2)^3+\cdots.
\end{equation}

\noindent The Fourier transform of the Laplace operator $\nabla^2$ is $-||\bm{w}||$, thus the Fourier transform of $S(||\bm{w}||)$ is
%
\begin{equation}\label{ch5_eq_K}
\mathcal{K}=a_0+a_1(-\nabla^2)+a_2(-\nabla^2)^2+a_3(-\nabla^2)^3+\cdots,
\end{equation}

\noindent defining a pseudo-differential operator as a series of Laplace operators.

If the negative Laplace operator $-\nabla^2$ is defined as the covariance operator of the formal kernel $l$,
%
\begin{equation*}
-\nabla^2 f(\bm{x}) = \int l(\bm{x},\bm{x}') f(\bm{x}') d\bm{x}',
\end{equation*} 

\noindent then the formal kernel can be represented as 
%
\begin{equation*}
l(\bm{x},\bm{x}')= \sum_j \lambda_j \phi_j(\bm{x}) \phi_j(\bm{x}'),
\end{equation*}

\noindent where $\{\lambda_j\}_{j=1}^{\infty}$ and $\{\phi_j(\bm{x})\}_{j=1}^{\infty}$ are the set of eigenvalues and eigenvectors, respectively, of the Laplacian operator. Namely, they satisfy the following eigenvalue problem in the compact subset $\bm{x} \in \Omega \subset {\rm I\!R}^D$ and with the Dirichlet boundary condition (another boundary condition could be used as well):
%
\begin{align*}
-\nabla^2 \phi_j(\bm{x})&=\lambda \phi_j(\bm{x}), \hspace{1cm}  x\in \Omega \nonumber \\ 
\phi_j(\bm{x})&=0, \hspace{1.85cm} x\notin \Omega.
\end{align*}  

\noindent Because $-\nabla^2$ is a positive definite Hermitian operator, the set of eigenfunctions $\phi_j(\cdot)$ are orthonormal with respect to the inner product
%
\begin{eqnarray*}
<f,g>=\int_\Omega f(\bm{x}) g(\bm{x}) d(\bm{x})
\end{eqnarray*} 

\noindent that is,
%
\begin{eqnarray*}
\int_\Omega \phi_i(\bm{x}) \phi_j(\bm{x}) d(\bm{x}) = \delta_{ij},
\end{eqnarray*} 

\noindent and all the eigenvalues $\lambda_j$ are real and positive. 

Due to normality of the basis of the representation of the formal kernel $l(\bm{x},\bm{x}')$, its formal powers $s=1,2,\dots$ can be write as
%
\begin{eqnarray}\label{ch5_eq_formalkernel}
l(\bm{x},\bm{x}')^s= \sum_j \lambda_j^s \phi_j(\bm{x}) \phi_j(\bm{x}'),
\end{eqnarray} 

\noindent which are again to be interpreted to mean that
%
\begin{equation*}
(-\nabla^2)^s f(\bm{x}) = \int l^s(\bm{x},\bm{x}') f(\bm{x}') d\bm{x}'.
\end{equation*} 

\noindent This implies that we also have
%
\begin{equation*}
[a_0+a_1(-\nabla^2)+a_2(-\nabla^2)^2+\cdots] f(\bm{x}) = \int [a_0+a_1l^1(\bm{x},\bm{x}')+a_2l^2(\bm{x},\bm{x}')+\cdots] f(\bm{x}') d\bm{x}'.
\end{equation*} 


Then, looking at equations (\ref{ch5_eq_K}) and (\ref{ch5_eq_formalkernel}), it can be concluded 
%
\begin{eqnarray}\label{ch5_eq_k}
k(\bm{x},\bm{x}')= \sum_j [a_0+a_1\lambda_j^1+a_2\lambda_j^2+\cdots] \phi_j(\bm{x}) \phi_j(\bm{x}').
\end{eqnarray} 

\noindent By letting $||\bm{w}||^2=\lambda_j$ the spectral density in Equation (\ref{ch5_eq_S}) becomes
%
\begin{equation*}
S(\sqrt{\lambda_j})=a_0+a_1\lambda_j+a_2\lambda_j^2+a_3\lambda_j^3+\cdots,
\end{equation*}

\noindent and substituting in equation (\ref{ch5_eq_k}) then leads to the final searched approximation
%
\begin{eqnarray}\label{ch5_eq_k_2}
k(\bm{x},\bm{x}')= \sum_j S(\sqrt{\lambda_j}) \phi_j(\bm{x}) \phi_j(\bm{x}'),
\end{eqnarray} 

\noindent where $S(\cdot)$ is the spectral density of the covariance function, $\lambda_j$ is the $j$th eigenvalue and $\phi_j(\cdot)$ the eigenfunction of the Laplace operator in a given domain.



\section{Low-rank Gaussian process with a periodic covariance function}\label{ch5_sec_periodic}

A GP model with a periodic covariance function does no fit under the framework of the HSGP approximation covered so far in this study. However, it do has a low-rank representation. In this section, we first give a brief presentation of the results from \cite{solin2014explicit}, where the authors obtain an approximate linear representation of a periodic squared exponential covariance function based on expanding the periodic covariance function into a series of stochastic resonators. Secondly, we analyze the accuracy of this approximation and, finally, we derive the GP model with this approximate periodic square exponential covariance function.

The periodic squared exponential covariance function takes the form
%
\begin{eqnarray} \label{ch5_eq_cov_periodic}
k(\bm{\tau})= \sigma^2 \exp\Big(-\frac{2\text{sin}^2\left(\omega_0\frac{\bm{\tau}}{2}\right)}{\ell^2}\Big),
\end{eqnarray}

\noindent where $\sigma^2$ is the magnitude scale of the covariance, $\ell$ is the characteristic lengthscale of the covariance, and $\omega_0$ is the angular frequency defining the periodicity. 

In \cite{solin2014explicit}, the authors come to a cosine series expansion for the periodic covariance function (\ref{ch5_eq_cov_periodic}) as follows,
% 
\begin{eqnarray} \label{ch5_eq_cov_periodic_taylor_approx}
k(\tau)= \sigma^2 \sum_{j=0}^{J} \tilde{q}_j^2 \text{cos}(j\omega_0 \tau),
\end{eqnarray}

\noindent which comes basically from a Taylor series representation of the periodic covariance function. The coefficients $\tilde{q}_j^2$ of the previous expression are
%
\begin{eqnarray} \label{ch5_eq_q}
\tilde{q}_j^2= \frac{2}{\exp\left(\frac{1}{\ell^2}\right)} \sum_{j=0}^{\lfloor \frac{J-j}{2} \rfloor} \frac{(2\ell^2)^{-j-2}}{(j+i)!i!},
\end{eqnarray}

\noindent where $j=1,2,\cdots,J$, and $\lfloor \cdot \rfloor$ denotes the floor round-off operator. For the index $j=0$, the coefficient is 
%
\begin{eqnarray} \label{ch5_eq_q0}
\tilde{q}_0^2= \frac{1}{2} \frac{2}{\exp\left(\frac{1}{\ell^2}\right)} \sum_{j=0}^{\lfloor \frac{J-j}{2} \rfloor} \frac{(2\ell^2)^{-j-2}}{(j+i)!i!}.
\end{eqnarray}
 
\noindent Note that the covariance in equation (\ref{ch5_eq_cov_periodic_taylor_approx}) is a $J$th order truncation of a Taylor series representation. As \cite{solin2014explicit} argue, this approximation converges to equation (\ref{ch5_eq_cov_periodic}) when $J \rightarrow \infty$.

%If the requirement of a valid covariance function is relaxed and only an optimal series approximation is required, taking the limit $J \rightarrow \infty$ in the sub-sums in equation (\ref{ch5_eq_q}) gives the following variance coefficients
An upper bounded approximation to the coefficients $\tilde{q}_j^2$ and $\tilde{q}_0^2$ can be obtained by taking the limit $J \rightarrow \infty$ in the sub-sums in the corresponding equations (\ref{ch5_eq_q}) and (\ref{ch5_eq_q0}), and thus leading to the following variance coefficients:
%
\begin{equation}\label{ch5_eq_q_2}
\begin{split}
\tilde{q}_j^2= \frac{2\text{I}_j(\ell^{-2})}{\exp\left(\frac{1}{\ell^2}\right)}, \\
\tilde{q}_0^2= \frac{\text{I}_0(\ell^{-2})}{\exp\left(\frac{1}{\ell^2}\right)},
\end{split}
\end{equation} 

\noindent for $j=1,2,\cdots,J$, and where the $\text{I}_{\alpha}(z)$ is the modified Bessel function \citep{handbook1970m} of the first order of $\alpha$. This approximation implies that the requirement of a valid covariance function is relaxed and only an optimal series approximation is required \citep{solin2014explicit}. A more detailed explanation and mathematical proofs of this approximation of a periodic covariance function can be found in \cite{solin2014explicit}. 

%Note that the terms in equations (\ref{ch5_eq_q}) and (\ref{ch5_eq_q0}) are underneath bounded approximations for any $J$. 


In order to assess the accuracy of this representation as a function of the number of cosine terms $J$ considered in the approximation, an empirical evaluation is carried out in a similar way than that in Section \ref{ch5_sec_accuracy} of this work. Thus, Figure \ref{ch5_fig28_m_lscale_periodic} shows the minimum number of terms $J$ required to achieve a close approximation to the exact periodic squared exponential kernel as a function of the lengthscale of the kernel. We have considered an approximation to be close enough in terms of satisfying equation (\ref{ch5_eq_diff_covs}) with $\varepsilon=0.5\%$. Notice that since this is a series expansion of sinusoidal functions, the approximation does not depend on any boundary condition.

\begin{figure}
\centering
\subfigure{\includegraphics[scale=0.40, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig28_m_lscale_periodic_appendix.pdf}}
\hspace{3mm}
\subfigure{\includegraphics[scale=0.40, trim = 0mm 0mm 5mm 0mm, clip]{ch5_fig28_m_lscale_periodic_appendix_zoom.pdf}}
\caption{Relation among the minimum number of terms $J$ in the approximation and the lengthscale ($\ell$) of the periodic squared exponential covariance function. The right-side plot is a zoom in of the left-side plot.}
  \label{ch5_fig28_m_lscale_periodic}
\end{figure}


The function values of a GP model with this low-rank representation of the periodic exponential covariance function can be easily derived. Considering the identity
\begin{equation*}
\text{cos}(j\omega_0 (x-x'))=\text{cos}(j\omega_0 x) \text{cos}(j\omega_0 x') + \text{sin}(j\omega_0 x) \text{sin}(j\omega_0 x'),
\end{equation*}

\noindent the covariance $k(\tau)$ in equation (\ref{ch5_eq_cov_periodic_taylor_approx}) can be re-writting as
%
\begin{eqnarray} \label{ch5_eq_cov_periodic_taylor_approx_2}
k(x,x')= \sigma^2 \Big( \sum_{j=0}^{J} \tilde{q}_j^2 \text{cos}(j\omega_0 x)  \text{cos}(j\omega_0 x') + \sum_{j=1}^{J} \tilde{q}_j^2 \text{sin}(j\omega_0 x) \text{sin}(j\omega_0 x') \Big).
\end{eqnarray}

\noindent where $\tau=x-x'$. With this approximation for the periodic squared exponential covariance function $k(x,x')$, the approximate GP model $f(x) \sim \mathcal{GP}(0,k(x,x')$ equivalently leads to a linear representation of $f(\cdot)$ via
%
\begin{equation} \label{ch5_eq_f_period}
f(x) \approx \sigma \Big( \sum_{j=0}^J  \tilde{q}_j \text{cos}(j\omega_0 x) \beta_j +  \sum_{j=1}^J \tilde{q}_j \text{sin}(j\omega_0 x) \beta_{J+1+j} \Big),
\end{equation}

\noindent where $\beta_j \sim \text{Normal}(0,1)$, with $j=1,\dots,2J+1$. The cosine $\text{cos}(j\omega_0 x)$ and sinus $\text{sin}(j\omega_0 x)$ terms do not depend on the covariance hyperparameters $\ell$. The only dependence on the hyperparameter $\ell$ is through the coefficients $\tilde{q}_j$, which are $J$-dimensional. The computational cost of this approximation scales as $O\big(n(2J+1) + (2J+1)\big)$, where $n$ is the number of observations and $J$ the number of cosine terms.

The parameterization in equation (\ref{ch5_eq_f_period}) is naturally in the non-centered
parameterization form with independent prior distribution on
$\beta_j$, which makes the posterior inference easier.



\section*{Acknowledgment}


\bibliography{references}


\end{document}
